{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"../resources/processed-goldstandard-XMLTXT.tsv\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to tokenize, remove stop words, get stemms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "def tokenizePorter(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for item in tokens:\n",
    "        if item not in stopWords: \n",
    "            stems.append(stemmer.stem(item))\n",
    "    return ' '.join(stems)\n",
    "\n",
    "def tokenizeSnowball(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    for item in tokens:\n",
    "        if item not in stopWords: \n",
    "            stems.append(stemmer.stem(item))\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Preprocessing the Text\n",
    "removePunctuation = str.maketrans('\\n', ' ', string.punctuation)\n",
    "\n",
    "# Transforms the text to lower case, remove punctuations, get the stemms of words \n",
    "data['title_abstract_mesh'] = data[['title', 'abstract', \"major_mesh\", \"minor_mesh\"]].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1)\n",
    "data['title_abstract_mesh_stemmed'] = data['title_abstract_mesh'].apply(tokenizeSnowball)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Sets First into PM and not PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmSet = data[data['pm_rel_desc'].str.contains('Human PM|Animal PM', regex=True)]\n",
    "pmSet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not PM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notPmSet = data[data['pm_rel_desc'].str.contains('Not PM', regex=True)]\n",
    "notPmSet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfMeanWeight(data):\n",
    "    tvec = TfidfVectorizer()\n",
    "    tvecWeights = tvec.fit_transform(data['title_abstract_mesh_stemmed'])\n",
    "\n",
    "    weights = np.asarray(tvecWeights.mean(axis=0)).ravel().tolist()\n",
    "    weightsDf = pd.DataFrame({'term': tvec.get_feature_names(), 'weight': weights})\n",
    "    return weightsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsPM = tfidfMeanWeight(pmSet)\n",
    "topPM = weightsPM.sort_values(by='weight', ascending=False).head(10)\n",
    "topPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsNotPM = tfidfMeanWeight(notPmSet)\n",
    "topNotPM = weightsNotPM.sort_values(by='weight', ascending=False).head(10)\n",
    "topNotPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedOnlyPM = pd.merge(topPM, topNotPM, on=\"term\", how=\"outer\", suffixes=[\"_pm\", \"_notpm\"])\n",
    "mergedOnlyPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting DataSets Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: https://buhrmann.github.io/tfidf-analysis.html\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=100):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_feats_by_class(Weights, data, features, min_tfidf=0.1, top_n=100):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(data)\n",
    "    for label in labels:\n",
    "        ids = np.where(data==label)\n",
    "        feats_df = top_mean_feats(Weights, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs\n",
    "\n",
    "def top_feats_pm_notpm(Weights, data, features, min_tfidf=0.1, top_n=50):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "\n",
    "    ids = np.where(data==\"Human PM\") or np.where(data==\"Animal PM\")\n",
    "    feats_df = top_mean_feats(Weights, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "    feats_df.label = \"PM\"\n",
    "    dfs.append(feats_df)\n",
    "\n",
    "    ids = np.where(data==\"Not PM\")\n",
    "    feats_df = top_mean_feats(Weights, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "    feats_df.label = \"Not PM\"\n",
    "    dfs.append(feats_df)\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(max_features=50000)\n",
    "tvecWeights = vec.fit_transform(data['title_abstract_mesh_stemmed'])\n",
    "features = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human PM , Animal PM , Not PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = top_feats_by_class(tvecWeights, data[\"pm_rel_desc\"], features)\n",
    "newDict = {}\n",
    "for df in dfs:\n",
    "    print(df.label)\n",
    "    print(df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "mergedPm = pd.merge(dfs[0], dfs[1], on=\"feature\", how=\"outer\", suffixes=[\"_animal\", \"_human\"])\n",
    "mergedPmNotPm = pd.merge(mergedPm, dfs[2], on=\"feature\", how=\"outer\", suffixes=[\"_pm\", \"_not_pm\"])\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 5):\n",
    "    display(mergedPmNotPm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PM and NOT PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = top_feats_pm_notpm(tvecWeights, data[\"pm_rel_desc\"], features)\n",
    "newDict = {}\n",
    "\n",
    "for df in dfs:\n",
    "    print(df.label)\n",
    "    print(df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "merged = pd.merge(dfs[0], dfs[1], on=\"feature\", how=\"outer\", suffixes=[\"_pm\", \"_notpm\"])\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 5):\n",
    "    display(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = top_feats_by_class(tvecWeights, data[\"relevance_score\"], features)\n",
    "for df in dfs:\n",
    "    print(df.label)\n",
    "    print(df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = top_feats_by_class(tvecWeights, data[\"trec_topic_disease\"], features)\n",
    "for df in dfs:\n",
    "    print(df.label)\n",
    "    print(df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic + PM and NOT PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topic_pm'] = data[['trec_topic_disease', 'pm_rel_desc']].apply(lambda x: ''.join(x.to_string(index=False).replace(\"\\n\",\" \")), axis=1)\n",
    "data.head()\n",
    "\n",
    "dfs = top_feats_by_class(tvecWeights, data[\"topic_pm\"], features)\n",
    "for df in dfs:\n",
    "    print(df.label)\n",
    "    print(df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic + Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topic_relevance'] = data[['trec_topic_disease', 'relevance_score']].apply(lambda x: ''.join(x.to_string(index=False).replace(\"\\n\",\" \")), axis=1)\n",
    "data.head()\n",
    "\n",
    "dfs = top_feats_by_class(tvecWeights, data[\"topic_relevance\"], features)\n",
    "for df in dfs:\n",
    "    print(df.label)\n",
    "    print(df)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
