{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "* Topics from 2017: http://trec-cds.appspot.com/topics2017.xml\n",
    "* Topics from 2018: http://trec-cds.appspot.com/topics2018.xml\n",
    "* Pre-processed Gold-Standard or Run Files that can be found on fighsare:\n",
    "    * 2017 GS: @todo\n",
    "    * 2018 GS: @todo\n",
    "    * Example of Run File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import json\n",
    "import gzip\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import warnings\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING pyltr with LETOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"/path/MQ2007/Fold1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(join(folder,'train.txt')) as trainfile, open(join(folder,'vali.txt')) as valifile, open(join(folder,'test.txt')) as evalfile:\n",
    "    TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "    VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n",
    "    EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = pyltr.metrics.NDCG(k=10)\n",
    "\n",
    "# Only needed if you want to perform validation (early stopping & trimming)\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(VX, Vy, Vqids, metric=metric, stop_after=250)\n",
    "\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=metric,\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.02,\n",
    "    max_features=0.5,\n",
    "    query_subsample=0.5,\n",
    "    max_leaf_nodes=10,\n",
    "    min_samples_leaf=64,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model.fit(TX, Ty, Tqids, monitor=monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epred = model.predict(EX)\n",
    "print('Random ranking:', metric.calc_mean_random(Eqids, Ey))\n",
    "print('Our model:', metric.calc_mean(Eqids, Ey, Epred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = \"../train-path\"\n",
    "testPath = \"../test-path\"\n",
    "\n",
    "trainYear = \"2017\"\n",
    "testYear = \"2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsTrainFile = join(trainPath ,\"train-file.tsv\")\n",
    "trecEvalTrain = \"../topics2017.xml\"\n",
    "\n",
    "gsTestFile = join(testPath,\"test-file.tsv\")\n",
    "trecEvalTest = \"../topics2018.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Topics for Training Set (get gene info)\n",
    "topicsColumns = ['trec_topic_number', 'trec_topic_gene']\n",
    "topics = pd.DataFrame(columns=topicsColumns)\n",
    "topicsXML = etree.parse(trecEvalTrain)\n",
    "for topic in topicsXML.getroot():\n",
    "    topicNumber = topic.get('number')\n",
    "    gene = topic.find('gene').text\n",
    "    topics = topics.append(pd.Series([topicNumber, gene], index=topicsColumns), ignore_index=True)\n",
    "topics['trec_topic_number'] = topics['trec_topic_number'].astype('int')\n",
    "\n",
    "# Merging\n",
    "train = pd.read_csv(gsTrainFile, sep = '\\t', encoding='utf8')\n",
    "train.fillna(\"\", inplace=True)\n",
    "trainData = train.merge(topics, left_on=['trec_topic_number'], right_on=['trec_topic_number'], how='left')\n",
    "trainData.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Topics for Test Set (get gene info)\n",
    "topicsColumns = ['trec_topic_number', 'trec_topic_gene']\n",
    "topics = pd.DataFrame(columns=topicsColumns)\n",
    "topicsXML = etree.parse(trecEvalTest)\n",
    "for topic in topicsXML.getroot():\n",
    "    topicNumber = topic.get('number')\n",
    "    gene = topic.find('gene').text\n",
    "    topics = topics.append(pd.Series([topicNumber, gene], index=topicsColumns), ignore_index=True)\n",
    "topics['trec_topic_number'] = topics['trec_topic_number'].astype('int')\n",
    "\n",
    "# Merging\n",
    "testVal = pd.read_csv(gsTestFile, sep = '\\t', encoding='utf8', dtype={'trec_doc_id':object})\n",
    "testVal.fillna(\"\", inplace=True)\n",
    "testValData = testVal.merge(topics, left_on=['trec_topic_number'], right_on=['trec_topic_number'], how='left')\n",
    "testValData.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to tokenize, remove stop words, get stemms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopWords = stopwords.words('english')\n",
    "\n",
    "def tokenizePorter(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for item in tokens:\n",
    "        if item not in stopWords: \n",
    "            stems.append(stemmer.stem(item))\n",
    "    return ' '.join(stems)\n",
    "\n",
    "def tokenizeSnowball(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    for item in tokens:\n",
    "        if item not in stopWords: \n",
    "            stems.append(stemmer.stem(item))\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Preprocessing the Text\n",
    "removePunctuation = str.maketrans('\\n', ' ', string.punctuation)\n",
    "\n",
    "# Transforms the text to lower case, remove punctuations, get the stemms of words \n",
    "trainData['title_abstract_mesh'] = trainData[['title', 'abstract', \"major_mesh\", \"minor_mesh\"]].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1)\n",
    "trainData['title_abstract_mesh_stemmed'] = trainData['title_abstract_mesh'].apply(tokenizeSnowball)\n",
    "trainData['title_stemmed'] = trainData[['title']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "trainData['abstract_stemmed'] = trainData[['abstract']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "trainData['mesh_stemmed'] = trainData[['major_mesh', 'minor_mesh']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "trainData['disease_stemmed'] = trainData[['trec_topic_disease']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "trainData['gene_stemmed'] = trainData[['trec_topic_gene']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "\n",
    "# Defining query ids\n",
    "trainData[\"qid\"] = trainData[\"trec_topic_number\"]\n",
    "# trainData[\"qid\"] = trainData[\"trec_topic_number\"].astype(str)+str(trainYear)\n",
    "trainDataSliced = trainData[['relevance_score','qid', 'title_stemmed', 'abstract_stemmed', 'mesh_stemmed', 'title_abstract_mesh_stemmed', 'disease_stemmed', 'gene_stemmed', 'trec_doc_id']]\n",
    "trainDataSliced.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced = []\n",
    "valDataSetSliced = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "# Preprocessing the Text\n",
    "removePunctuation = str.maketrans('\\n', ' ', string.punctuation)\n",
    "\n",
    "# Transforms the text to lower case, remove punctuations, get the stemms of words \n",
    "testValData['title_abstract_mesh'] = testValData[['title', 'abstract', \"major_mesh\", \"minor_mesh\"]].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1)\n",
    "testValData['title_abstract_mesh_stemmed'] = testValData['title_abstract_mesh'].apply(tokenizeSnowball)\n",
    "testValData['title_stemmed'] = testValData[['title']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "testValData['abstract_stemmed'] = testValData[['abstract']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "testValData['mesh_stemmed'] = testValData[['major_mesh', 'minor_mesh']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "testValData['disease_stemmed'] = testValData[['trec_topic_disease']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "testValData['gene_stemmed'] = testValData[['trec_topic_gene']].apply(lambda x: ''.join(re.sub(r';|\\/', ' ', x.to_string(index=False).lower()).translate(removePunctuation)), axis=1).apply(tokenizeSnowball)\n",
    "\n",
    "# Defining query ids\n",
    "testValData[\"qid\"] = testValData[\"trec_topic_number\"]\n",
    "testValDataSliced = testValData[['relevance_score','qid', 'title_stemmed', 'abstract_stemmed', 'mesh_stemmed', 'title_abstract_mesh_stemmed', 'disease_stemmed', 'gene_stemmed', 'trec_doc_id']]\n",
    "testValDataSliced.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for Disease and Gene in Title, Abstract, Mesh and Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countTerms(terms, target):\n",
    "    vectorizer = CountVectorizer(vocabulary = terms)\n",
    "    transformed_data = vectorizer.fit_transform(target)\n",
    "\n",
    "    score = pd.DataFrame(transformed_data.toarray(), columns=vectorizer.get_feature_names())\n",
    "    scoreDict = score.to_dict('records')\n",
    "    return scoreDict\n",
    "    \n",
    "def specificTermsCount(allTermsCount, index, terms):\n",
    "    termsCount = 0\n",
    "    termList = terms.split()\n",
    "    for term in termList:\n",
    "        termsCount += allTermsCount[index][term]\n",
    "    return(termsCount)\n",
    "\n",
    "def percentageOfTermsMatched(allTermsCount, index, terms):\n",
    "    matched = 0\n",
    "    termList = terms.split()\n",
    "    for term in termList:\n",
    "        if allTermsCount[index][term] > 0:\n",
    "            matched += 1\n",
    "    return(matched/len(termList))\n",
    "\n",
    "def termsFrequency(corpus, termCount):\n",
    "    count = len(corpus.split())\n",
    "    if count == 0:\n",
    "        return 0\n",
    "    return float(termCount/count)\n",
    "\n",
    "def tfidfWeights(terms, target):\n",
    "    tvec = TfidfVectorizer(vocabulary = terms)\n",
    "    weights = tvec.fit_transform(target)\n",
    "    \n",
    "    score = pd.DataFrame(weights.toarray(), columns=tvec.get_feature_names())\n",
    "    scoreDict = score.to_dict('records')\n",
    "    return scoreDict\n",
    "\n",
    "def specificTermsTfIdf(tfidfWeights, index, terms):\n",
    "    tfidf = 0\n",
    "    termList = terms.split()\n",
    "    for term in termList:\n",
    "        tfidf += tfidfWeights[index][term]\n",
    "    return(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSelectedFeatures ={'disease_title_count': 1,\n",
    "             'disease_title_tf': 2,\n",
    "             'disease_title_percent': 3,\n",
    "             'disease_title_tfidf': 4,\n",
    "             'gene_title_count': 5,\n",
    "             'gene_title_tf': 6,\n",
    "             'gene_title_percent': 7,\n",
    "             'gene_title_tfidf': 8,\n",
    "             'disease_gene_title_tf': 9,\n",
    "             'disease_gene_title_tfidf': 10,\n",
    "             'disease_abstract_count': 11,\n",
    "             'disease_abstract_tf': 12,\n",
    "             'disease_abstract_percent': 13,\n",
    "             'disease_abstract_tfidf': 14,\n",
    "             'gene_abstract_count': 15,\n",
    "             'gene_abstract_tf': 16,\n",
    "             'gene_abstract_percent': 17,\n",
    "             'gene_abstract_tfidf': 18,\n",
    "             'disease_gene_abstract_tf': 19,\n",
    "             'disease_gene_abstract_tfidf': 20,\n",
    "             'disease_mesh_count': 21,\n",
    "             'disease_mesh_tf': 22,\n",
    "             'disease_mesh_percent': 23,\n",
    "             'disease_mesh_tfidf': 24,\n",
    "             'gene_mesh_count': 25,\n",
    "             'gene_mesh_tf': 26,\n",
    "             'gene_mesh_percent': 27,\n",
    "             'gene_mesh_tfidf': 28,\n",
    "             'disease_gene_mesh_tf': 29,\n",
    "             'disease_gene_mesh_tfidf': 30,\n",
    "             'disease_combined_count': 31,\n",
    "             'disease_combined_tf': 32,\n",
    "             'disease_combined_percent': 33,\n",
    "             'disease_combined_tfidf': 34,\n",
    "             'gene_combined_count': 35,\n",
    "             'gene_combined_tf': 36,\n",
    "             'gene_combined_percent': 37,\n",
    "             'gene_combined_tfidf': 38,\n",
    "             'disease_gene_combined_tf': 39,\n",
    "             'disease_gene_combined_tfidf': 40\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures (diseaseTerms, geneTerms, trainDataSliced):\n",
    "    # TITLE\n",
    "    \n",
    "    # Disease\n",
    "    termsCountTitle = countTerms(diseaseTerms, trainDataSliced['title_stemmed'])\n",
    "\n",
    "    trainDataSliced['disease_title_count'] = trainDataSliced.apply(lambda row: specificTermsCount(termsCountTitle, row.name, row['disease_stemmed']), axis=1)\n",
    "    trainDataSliced['disease_title_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['title_stemmed'], row['disease_title_count']), axis=1)\n",
    "    trainDataSliced['disease_title_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(termsCountTitle, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfTitle = tfidfWeights(diseaseTerms, trainDataSliced['title_stemmed'])\n",
    "    trainDataSliced['disease_title_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfTitle, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "\n",
    "    # Gene\n",
    "    geneCountTitle = countTerms(geneTerms, trainDataSliced['title_stemmed'])\n",
    "\n",
    "    trainDataSliced['gene_title_count'] = trainDataSliced.apply(lambda row: specificTermsCount(geneCountTitle, row.name, row['gene_stemmed']), axis=1)\n",
    "    trainDataSliced['gene_title_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['title_stemmed'], row['gene_title_count']), axis=1)\n",
    "    trainDataSliced['gene_title_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(geneCountTitle, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfTitle = tfidfWeights(geneTerms, trainDataSliced['title_stemmed'])\n",
    "    trainDataSliced['gene_title_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfTitle, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    # Disease and Gene\n",
    "    trainDataSliced['disease_gene_title_tf'] = trainDataSliced['disease_title_tf'] + trainDataSliced['gene_title_tf']\n",
    "    trainDataSliced['disease_gene_title_tfidf'] = trainDataSliced['disease_title_tfidf'] + trainDataSliced['gene_title_tfidf']\n",
    "\n",
    "    # ABSTRACT\n",
    "\n",
    "    # Disease\n",
    "    termsCountAbst = countTerms(diseaseTerms, trainDataSliced['abstract_stemmed'])\n",
    "\n",
    "    trainDataSliced['disease_abstract_count'] = trainDataSliced.apply(lambda row: specificTermsCount(termsCountAbst, row.name, row['disease_stemmed']), axis=1)\n",
    "    trainDataSliced['disease_abstract_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['abstract_stemmed'], row['disease_abstract_count']), axis=1)\n",
    "    trainDataSliced['disease_abstract_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(termsCountAbst, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfAbstract = tfidfWeights(diseaseTerms, trainDataSliced['abstract_stemmed'])\n",
    "    trainDataSliced['disease_abstract_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfAbstract, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    # Gene\n",
    "    geneCountAbst = countTerms(geneTerms, trainDataSliced['abstract_stemmed'])\n",
    "\n",
    "    trainDataSliced['gene_abstract_count'] = trainDataSliced.apply(lambda row: specificTermsCount(geneCountAbst, row.name, row['gene_stemmed']), axis=1)\n",
    "    trainDataSliced['gene_abstract_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['abstract_stemmed'], row['gene_abstract_count']), axis=1)\n",
    "    trainDataSliced['gene_abstract_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(geneCountAbst, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfAbstract = tfidfWeights(geneTerms, trainDataSliced['abstract_stemmed'])\n",
    "    trainDataSliced['gene_abstract_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfAbstract, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    # Disease + Gene\n",
    "    trainDataSliced['disease_gene_abstract_tf'] = trainDataSliced['disease_abstract_tf'] + trainDataSliced['gene_abstract_tf']\n",
    "    trainDataSliced['disease_gene_abstract_tfidf'] = trainDataSliced['disease_abstract_tfidf'] + trainDataSliced['gene_abstract_tfidf']\n",
    "\n",
    "    # MESH\n",
    "\n",
    "    # Disease\n",
    "    termsCountMesh = countTerms(diseaseTerms, trainDataSliced['mesh_stemmed'])\n",
    "\n",
    "    trainDataSliced['disease_mesh_count'] = trainDataSliced.apply(lambda row: specificTermsCount(termsCountMesh, row.name, row['disease_stemmed']), axis=1)\n",
    "    trainDataSliced['disease_mesh_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['mesh_stemmed'], row['disease_mesh_count']), axis=1)\n",
    "    trainDataSliced['disease_mesh_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(termsCountMesh, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfMesh = tfidfWeights(diseaseTerms, trainDataSliced['mesh_stemmed'])\n",
    "    trainDataSliced['disease_mesh_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfMesh, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    # Gene\n",
    "    geneCountMesh = countTerms(geneTerms, trainDataSliced['mesh_stemmed'])\n",
    "\n",
    "    trainDataSliced['gene_mesh_count'] = trainDataSliced.apply(lambda row: specificTermsCount(geneCountMesh, row.name, row['gene_stemmed']), axis=1)\n",
    "    trainDataSliced['gene_mesh_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['mesh_stemmed'], row['gene_mesh_count']), axis=1)\n",
    "    trainDataSliced['gene_mesh_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(geneCountMesh, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfMesh = tfidfWeights(geneTerms, trainDataSliced['mesh_stemmed'])\n",
    "    trainDataSliced['gene_mesh_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfMesh, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    # Disease + Gene\n",
    "    trainDataSliced['disease_gene_mesh_tf'] = trainDataSliced['disease_mesh_tf'] + trainDataSliced['gene_mesh_tf']\n",
    "    trainDataSliced['disease_gene_mesh_tfidf'] = trainDataSliced['disease_mesh_tfidf'] + trainDataSliced['gene_mesh_tfidf']\n",
    "\n",
    "    # COMBINED\n",
    "\n",
    "    # Disease\n",
    "    termsCountCombined = countTerms(diseaseTerms, trainDataSliced['title_abstract_mesh_stemmed'])\n",
    "\n",
    "    trainDataSliced['disease_combined_count'] = trainDataSliced.apply(lambda row: specificTermsCount(termsCountCombined, row.name, row['disease_stemmed']), axis=1)\n",
    "    trainDataSliced['disease_combined_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['title_abstract_mesh_stemmed'], row['disease_combined_count']), axis=1)\n",
    "    trainDataSliced['disease_combined_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(termsCountCombined, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfCombined = tfidfWeights(diseaseTerms, trainDataSliced['title_abstract_mesh_stemmed'])\n",
    "    trainDataSliced['disease_combined_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfCombined, row.name, row['disease_stemmed']), axis=1)\n",
    "\n",
    "    # Gene\n",
    "    geneCountCombined = countTerms(geneTerms, trainDataSliced['title_abstract_mesh_stemmed'])\n",
    "\n",
    "    trainDataSliced['gene_combined_count'] = trainDataSliced.apply(lambda row: specificTermsCount(geneCountCombined, row.name, row['gene_stemmed']), axis=1)\n",
    "    trainDataSliced['gene_combined_tf'] = trainDataSliced.apply(lambda row: termsFrequency(row['title_abstract_mesh_stemmed'], row['gene_combined_count']), axis=1)\n",
    "    trainDataSliced['gene_combined_percent'] = trainDataSliced.apply(lambda row: percentageOfTermsMatched(geneCountCombined, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    termsTfIdfCombined = tfidfWeights(geneTerms, trainDataSliced['title_abstract_mesh_stemmed'])\n",
    "    trainDataSliced['gene_combined_tfidf'] = trainDataSliced.apply(lambda row: specificTermsTfIdf(termsTfIdfCombined, row.name, row['gene_stemmed']), axis=1)\n",
    "\n",
    "    # Disease + Gene\n",
    "    trainDataSliced['disease_gene_combined_tf'] = trainDataSliced['disease_combined_tf'] + trainDataSliced['gene_combined_tf']\n",
    "    trainDataSliced['disease_gene_combined_tfidf'] = trainDataSliced['disease_combined_tfidf'] + trainDataSliced['gene_combined_tfidf']\n",
    "\n",
    "    return trainDataSliced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features for Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = trainDataSliced['disease_stemmed'].unique()\n",
    "diseaseTerms = []\n",
    "for disease in diseases:\n",
    "    d = disease.split()\n",
    "    for word in d:\n",
    "        if word not in diseaseTerms:\n",
    "            diseaseTerms.append(word)\n",
    "print(diseaseTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allGenes = trainDataSliced['gene_stemmed'].unique()\n",
    "geneTerms = []\n",
    "for genes in allGenes:\n",
    "    d = genes.split()\n",
    "    for gene in d:\n",
    "        if gene not in geneTerms:\n",
    "            geneTerms.append(gene)\n",
    "print(geneTerms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: expansions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../lexigram-output.json') as f:\n",
    "    exPandedDisease = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = extractFeatures(diseaseTerms, geneTerms, trainDataSliced)\n",
    "pd.options.display.max_columns = None\n",
    "display(trainData.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDocId = trainData['trec_doc_id']\n",
    "croppedTrain = trainData.drop(['title_abstract_mesh_stemmed', 'title_stemmed', 'abstract_stemmed', \n",
    "                              'mesh_stemmed', 'disease_stemmed', 'gene_stemmed', 'trec_doc_id'], axis=1)\n",
    "\n",
    "finalTrain = croppedTrain.sort_values('qid')\n",
    "finalTrain['trec_doc_id'] = trainDocId\n",
    "finalTrain.head(1)\n",
    "\n",
    "rankTrain = finalTrain.to_dict('records')\n",
    "    \n",
    "f = open(\"train.txt\", \"w\")\n",
    "\n",
    "for item in rankTrain:\n",
    "    for i,val in item.items():\n",
    "        if(i == \"relevance_score\"):\n",
    "            f.write(str(val)+\" \")\n",
    "        elif(i == \"trec_doc_id\"):\n",
    "            f.write('# '+str(val))\n",
    "        elif(i == \"qid\"):\n",
    "            f.write(str(i)+\":\"+str(val)+\" \")\n",
    "        else:\n",
    "            j = allSelectedFeatures[i]\n",
    "            f.write(str(j)+\":\"+str(val)+\" \")\n",
    "    f.write(\"\\n\")    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for Test and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = testValDataSliced['disease_stemmed'].unique()\n",
    "diseaseTermsTest = []\n",
    "for disease in diseases:\n",
    "    d = disease.split()\n",
    "    for word in d:\n",
    "        if word not in diseaseTermsTest:\n",
    "            diseaseTermsTest.append(word)\n",
    "print(diseaseTermsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allGenes = testValDataSliced['gene_stemmed'].unique()\n",
    "geneTermsTest = []\n",
    "for genes in allGenes:\n",
    "    d = genes.split()\n",
    "    for gene in d:\n",
    "        if gene not in geneTermsTest:\n",
    "            geneTermsTest.append(gene)\n",
    "print(geneTermsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Testind and *no* Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testValDataSliced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced.append(testValDataSliced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Testing and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into validation and testing\n",
    "testDataSliced, valDataSliced, yT, yV = train_test_split(testValDataSliced, testValDataSliced['qid'], test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced.append(testDataSliced)\n",
    "valDataSetSliced.append(valDataSliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valDataSetSliced[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Test and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "testDataSetSliced = []\n",
    "valDataSetSliced = []\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "for train_index, test_index in kf.split(testValDataSliced):\n",
    "    testDataSetSliced.append(testValDataSliced.iloc[train_index])\n",
    "    valDataSetSliced.append(testValDataSliced.iloc[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataSetSliced[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valDataSetSliced[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = 1\n",
    "\n",
    "for testDataSliced in testDataSetSliced:\n",
    "    testDataSliced.reset_index(drop=True, inplace=True)\n",
    "    testData = extractFeatures(diseaseTermsTest, geneTermsTest, testDataSliced)\n",
    "    \n",
    "    testDocId = testData['trec_doc_id']\n",
    "    croppedTest = testData.drop(['title_abstract_mesh_stemmed', 'title_stemmed', 'abstract_stemmed', \n",
    "                              'mesh_stemmed', 'disease_stemmed', 'gene_stemmed', 'trec_doc_id'], axis=1)\n",
    "\n",
    "    finalTest = croppedTest.sort_values('qid')\n",
    "    finalTest['trec_doc_id'] = testDocId\n",
    "\n",
    "    rankTest = finalTest.to_dict('records')\n",
    "\n",
    "    f = open(\"test\"+str(sets)+\".txt\", \"w\")\n",
    "\n",
    "    for item in rankTest:\n",
    "        for i,val in item.items():\n",
    "            if(i == \"relevance_score\"):\n",
    "                f.write(str(val)+\" \")\n",
    "            elif(i == \"trec_doc_id\"):\n",
    "                f.write('# '+str(val))\n",
    "            elif(i == \"qid\"):\n",
    "                f.write(str(i)+\":\"+str(val)+\" \")\n",
    "            else:\n",
    "                j = allSelectedFeatures[i]\n",
    "                f.write(str(j)+\":\"+str(val)+\" \")\n",
    "        f.write(\"\\n\")    \n",
    "    f.close()\n",
    "    sets = sets + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = 1\n",
    "\n",
    "for valDataSliced in valDataSetSliced:\n",
    "    valDataSliced.reset_index(drop=True, inplace=True)\n",
    "    valData = extractFeatures(diseaseTermsTest, geneTermsTest, valDataSliced)\n",
    "    \n",
    "    valDocId = valData['trec_doc_id']\n",
    "    croppedVal = valData.drop(['title_abstract_mesh_stemmed', 'title_stemmed', 'abstract_stemmed', \n",
    "                              'mesh_stemmed', 'disease_stemmed', 'gene_stemmed', 'trec_doc_id'], axis=1)\n",
    "\n",
    "    finalVal = croppedVal.sort_values('qid')\n",
    "    finalVal['trec_doc_id'] = valDocId\n",
    "\n",
    "    rankVal = finalVal.to_dict('records')\n",
    "\n",
    "    f = open(\"val\"+str(sets)+\".txt\", \"w\")\n",
    "\n",
    "    for item in rankVal:\n",
    "        for i,val in item.items():\n",
    "            if(i == \"relevance_score\"):\n",
    "                f.write(str(val)+\" \")\n",
    "            elif(i == \"trec_doc_id\"):\n",
    "                f.write('# '+str(val))\n",
    "            elif(i == \"qid\"):\n",
    "                f.write(str(i)+\":\"+str(val)+\" \")\n",
    "            else:\n",
    "                j = allSelectedFeatures[i]\n",
    "                f.write(str(j)+\":\"+str(val)+\" \")\n",
    "        f.write(\"\\n\")    \n",
    "    f.close()\n",
    "    sets = sets + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "randonRanking = []\n",
    "ourRanking = []\n",
    "allFeatures = []\n",
    "\n",
    "while count < sets:\n",
    "    metric = pyltr.metrics.NDCG(k=10)\n",
    "\n",
    "    model = pyltr.models.LambdaMART(\n",
    "        metric=metric,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.02,\n",
    "        max_features=0.5,\n",
    "        query_subsample=0.5,\n",
    "        max_leaf_nodes=10,\n",
    "        min_samples_leaf=64,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    print(\"Fold: \"+str(count))\n",
    "    with open('train.txt') as trainfile, open('val'+str(count)+'.txt') as valifile, open('test'+str(count)+'.txt') as evalfile:\n",
    "        TrainX, Trainy, TrainQids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "        ValX, Valy, ValQids, _ = pyltr.data.letor.read_dataset(valifile)\n",
    "        EvalX, Evaly, EvalQids, _ = pyltr.data.letor.read_dataset(evalfile)\n",
    "        \n",
    "    monitor = pyltr.models.monitors.ValidationMonitor(ValX, Valy, ValQids, metric=metric, stop_after=250)\n",
    "    model.fit(TrainX, Trainy, TrainQids, monitor=monitor)\n",
    "    Epred = model.predict(EvalX)\n",
    "    randonRanking.append(metric.calc_mean_random(EvalQids, Evaly))\n",
    "    ourRanking.append(metric.calc_mean(EvalQids, Evaly, Epred))\n",
    "    \n",
    "    # features\n",
    "    nonZero = np.nonzero(model.feature_importances_)\n",
    "    for i in nonZero:\n",
    "        nonZeros = i.tolist()\n",
    "        \n",
    "    listFeatures = np.argsort(model.feature_importances_)\n",
    "        \n",
    "    for feature in listFeatures:\n",
    "        if (feature in nonZeros) and (feature not in allFeatures):\n",
    "            allFeatures.append(feature)\n",
    "    \n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epred = model.predict(EvalX)\n",
    "metric.calc_mean_random(EvalQids, Evaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpR = np.asarray(randonRanking)\n",
    "np.mean(numpR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ourRanking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpR = np.asarray(ourRanking)\n",
    "np.mean(numpR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in allFeatures:\n",
    "    for key in allSelectedFeatures:\n",
    "        f = feature+1\n",
    "        if allSelectedFeatures[key] == f:\n",
    "            print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
