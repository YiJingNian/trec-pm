{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "* Any Run in the TREC Format\n",
    "* Pubmed XML collection: http://trec-cds.appspot.com/2018.html#documents\n",
    "* 2018 Topics: http://trec-cds.appspot.com/topics2018.xml\n",
    "* Extra Abstracts TXT collection: http://trec-cds.appspot.com/2018.html#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import gzip\n",
    "import time\n",
    "import csv\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompress _.tar.gz_ Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompress Files\n",
    "def decompress(myPath):\n",
    "    fileNames = [f for f in listdir(myPath) if isfile(join(myPath, f)) and f[-7:] == \".tar.gz\"]\n",
    "    for file in fileNames:\n",
    "        print(\"Extracting from: \", file)\n",
    "        tar = tarfile.open(join(myPath, file), \"r:gz\")\n",
    "        tar.extractall(join(myPath, file[:-7]))\n",
    "        tar.close()\n",
    "        print(\"Done\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # Path containing the medline_xml.part[x].tar.gz files (Pubmed XML collection)\n",
    "    pubMedAbstracts = \"../TREC/XML-Collection\"\n",
    "\n",
    "    # Decompress files\n",
    "    decompress(pubMedAbstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Information from Pubmed XML and Extra Abstracts Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Pubmed Ids from the CSV file\n",
    "def extractDocIDs(filePath):\n",
    "    \"\"\" Extracts all ids from the gold standard \"\"\"\n",
    "    f = pd.read_csv(filePath, \n",
    "                    names=[\"trec_topic_number\", \"x\", \"trec_doc_id\", \"order\", \"relevance\", \"name\"], dtype={'trec_doc_id':object},\n",
    "                    sep=\"\\t\", encoding=\"utf-8\"\n",
    "                   )\n",
    "    return set(f['trec_doc_id'])\n",
    "\n",
    "# Get the name of the folders containing xml.gz files\n",
    "def getFolderNames(myPath):\n",
    "    dirNames = [d for d in listdir(myPath) if isdir(join(myPath, d))]\n",
    "    return dirNames\n",
    "\n",
    "# Get the name of the xml.gz files\n",
    "def getGzFileNames(myPath):\n",
    "    fileNames = [f for f in listdir(myPath) if isfile(join(myPath, f)) and f[-7:] == \".xml.gz\"]\n",
    "    return fileNames\n",
    "\n",
    "def getTarFileNames(myPath):\n",
    "    fileNames = [f for f in listdir(myPath) if isfile(join(myPath, f)) and f[-4:] == \".tar\"]\n",
    "    return fileNames\n",
    "\n",
    "def unzipTar(folderPath, docIDsPath, targetFolder=''):\n",
    "    # Unzip either pubmed or extra abstracts from folderPath to targetFolder if they are in the gold standard\n",
    "    ids = extractDocIDs(docIDsPath)\n",
    "    print(\"Gold Standard Ids:\", len(ids))\n",
    "    tarFiles = getTarFileNames(folderPath)\n",
    "    print(tarFiles)\n",
    "    \n",
    "    if targetFolder:\n",
    "        outpuPath=join(folderPath, targetFolder)\n",
    "    else:\n",
    "        outpuPath=folderPath\n",
    "    txtCounter = 0\n",
    "    for tarFileName in tarFiles:\n",
    "        print(\"Searching through:\", tarFileName)\n",
    "        tar = tarfile.open(join(folderPath, tarFileName), 'r:')\n",
    "        for txtFile in tar:\n",
    "            \n",
    "            # Extract ID from full path\n",
    "            docID = re.search( r'\\/(.*)\\.', txtFile.name)\n",
    "            if docID:\n",
    "                # Extract file only when there is a match\n",
    "                if docID.group(1) in ids:\n",
    "                    txtCounter += 1\n",
    "                    ids.remove(docID.group(1))\n",
    "                    tar.extract(txtFile, path=outpuPath)\n",
    "\n",
    "        tar.close()\n",
    "    print(\"Matched files:\", txtCounter)\n",
    "        \n",
    "# Extract relevant information from the papers inside the XML files that match the gold-standard\n",
    "def extractFeatures(folderPath, docIDsPath, outputPath):\n",
    "    st = time.time()\n",
    "    \n",
    "    # Get Pubmed Ids from the Run\n",
    "    ids = extractDocIDs(docIDsPath)\n",
    "    print(\"Nr of PMIDs in the Gold-Standard:\", len(ids))\n",
    "    # Recover the names of each folder containing xml.gz files\n",
    "    \n",
    "    folderNames = getFolderNames(folderPath)\n",
    "    \n",
    "    nrExtractedXMLs = 0\n",
    "    \n",
    "    # Create CSV for the output\n",
    "    with open(outputPath, 'w', encoding='utf-8') as extractFile:\n",
    "        wr = csv.writer(extractFile, quoting=csv.QUOTE_ALL, delimiter=\"\\t\")\n",
    "        wr.writerow([\"trec_doc_id\",\"title\",\"abstract\",\"major_mesh\",\"minor_mesh\"])\n",
    "    \n",
    "    # Iterate through the folders with the xml.gz files\n",
    "    for folderName in folderNames:\n",
    "        print(\"Looking into files from folder: \", folderName)\n",
    "        gzFiles = getGzFileNames(join(folderPath, folderName))\n",
    "        for gzFileName in gzFiles:\n",
    "            print(\"Analyzing information from: \", gzFileName)\n",
    "            gzFilePath = join(join(folderPath,folderName), gzFileName)\n",
    "            parser = etree.XMLParser(encoding='utf-8', recover=True)\n",
    "            pubMedArticleSet = etree.parse(gzip.open(gzFilePath, 'rt', encoding='utf-8'), parser=parser).getroot()\n",
    "            for mc in pubMedArticleSet.iterfind('PubmedArticle/MedlineCitation'):\n",
    "                pmid = mc.find(\"PMID\").text\n",
    "                # Verify if the PMID is in the list of IDs from the Run\n",
    "                majorMeshTerms = []\n",
    "                minorMeshTerms = []\n",
    "                abstractList = []\n",
    "                if pmid in ids:\n",
    "                    # Remove found pmid from ids set\n",
    "                    ids.remove(pmid)\n",
    "                    \n",
    "                    print(\"Extracting info from the PMID: \", pmid)\n",
    "                    # Get title\n",
    "                    if mc.find(\"Article/ArticleTitle\") is not None:\n",
    "                        title = ''.join(mc.find(\"Article/ArticleTitle\").itertext())\n",
    "                    # Get abstract, including the structured ones\n",
    "                    if mc.find(\"Article/Abstract\") is not None:\n",
    "                        for abstractPiece in mc.findall(\"Article/Abstract/AbstractText\"):\n",
    "                            abstractList.append(''.join(abstractPiece.itertext()))\n",
    "                        abstract = ' '.join(abstractList)\n",
    "                    # Extracting major and minor mesh descriptors\n",
    "                    # Qualifiers - not taking into account major and minor attributes\n",
    "                    for meshTerm in mc.findall(\"MeshHeadingList/MeshHeading\"):\n",
    "                        qualifiers = []\n",
    "                        for qualifier in meshTerm.findall(\"QualifierName\"):\n",
    "                            qualifiers.append(meshTerm.find(\"DescriptorName\").text + \"/\" + qualifier.text)\n",
    "                        if not qualifiers:\n",
    "                            fullMesh = meshTerm.find(\"DescriptorName\").text\n",
    "                            if meshTerm.find(\"DescriptorName\").get(\"MajorTopicYN\") == \"Y\":\n",
    "                                majorMeshTerms.append(fullMesh)\n",
    "                            else:\n",
    "                                minorMeshTerms.append(fullMesh)\n",
    "                        else:\n",
    "                            if meshTerm.find(\"DescriptorName\").get(\"MajorTopicYN\") == \"Y\":\n",
    "                                majorMeshTerms.extend(qualifiers)\n",
    "                            else:\n",
    "                                minorMeshTerms.extend(qualifiers)\n",
    "                    majorMeshList = \";\".join(majorMeshTerms)\n",
    "                    minorMeshList = \";\".join(minorMeshTerms)\n",
    "                    \n",
    "                    # Write the result to CSV\n",
    "                    with open(outputPath, 'a', encoding='utf-8') as extractFile:\n",
    "                        wr = csv.writer(extractFile, quoting=csv.QUOTE_ALL, delimiter=\"\\t\")\n",
    "                        wr.writerow([pmid, title, abstract, majorMeshList, minorMeshList])\n",
    "                \n",
    "                    # Count the number of extracted papers\n",
    "                    nrExtractedXMLs += 1\n",
    "    \n",
    "    print(\"Number of papers with information extracted: \", nrExtractedXMLs)\n",
    "    end = time.time()\n",
    "    print(\"Run time: \", end-st)\n",
    "    \n",
    "def extractExtraFeatures(extraAbstracts, extractedFeaturesFile):\n",
    "    files = [fi for fi in listdir(extraAbstracts) if isfile(join(extraAbstracts, fi))]\n",
    "    fCount = 0\n",
    "    for fi in files:\n",
    "        fCount += 1\n",
    "        fiObj = open(join(extraAbstracts, fi), encoding=\"utf8\")\n",
    "        fId = fi[:-4]\n",
    "        lines = fiObj.readlines()\n",
    "        fullTitle = lines[1].strip()\n",
    "        title = re.search( r'(Title:) (.*)', fullTitle).group(2)\n",
    "        abstract = \"\"\n",
    "        for line in lines[2:]:\n",
    "            if line.strip():\n",
    "                abstract += line.strip() + \" \"\n",
    "        with open(extractedFeaturesFile, 'a', encoding=\"utf8\") as extractFile:\n",
    "            wr = csv.writer(extractFile, quoting=csv.QUOTE_ALL, delimiter=\"\\t\")\n",
    "            wr.writerow([fId, title, abstract])    \n",
    "    print(\"Extracted files:\", fCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path containing the Run File\n",
    "docIDPath = \"../results/runs/run.trec_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Path containing the medline_xml.partx folders - they need to be extracted first\n",
    "    pubMedAbstracts = \"../TREC/XML-Collection\"\n",
    "   \n",
    "    # Output file\n",
    "    outputPath = \"../resources/relevant-abstracts-pubrun.csv\"\n",
    "    \n",
    "    # Extract relevant information from the XML files\n",
    "    extractFeatures(pubMedAbstracts, docIDPath, outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Relevant Pubmed Abstracts Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstracts = pd.read_csv(\"../resources/relevant-abstracts-pubrun.csv\", sep='\\t', encoding=\"utf-8\", dtype={'trec_doc_id':object})\n",
    "abstracts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and Read Extra Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path containing the downloaded txt collection (extra abstracts)\n",
    "extraAbstractsPath = \"../TXT-Collection/extra_abstracts\"\n",
    "# Path containing the extracted txt collection (extra abstracts)\n",
    "extraAbstractsNewPath = join(extraAbstractsPath,\"extra_abstracts\")\n",
    "abstractsGzFiles = getGzFileNames(extraAbstractsPath)\n",
    "extractedFeaturesFile = \"../resources/relevant-abstracts-pubrun.csv\"\n",
    "\n",
    "for abstractsGzFile in abstractsGzFiles:\n",
    "    print(\"Extracting: \", abstractsGzFile)\n",
    "    subprocess.call(['gunzip', '-d', join(abstractsPath, abstractsGzFile)])\n",
    "    print(\"Done\")\n",
    "\n",
    "unzipTar(extraAbstractsPath, docIDPath)\n",
    "extractExtraFeatures(extraAbstractsNewPath, extractedFeaturesFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = pd.read_csv(\"../resources/relevant-abstracts-pubrun.csv\", sep='\\t', encoding=\"utf-8\", dtype={'trec_doc_id':object})\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Run File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = pd.read_csv(\"../results/runs/run.trec_results\", sep='\\t', encoding=\"utf-8\", header=None, \n",
    "                        names=[\"trec_topic_number\", \"x\", \"trec_doc_id\", \"order\", \"relevance_score\", \"run_name\"], dtype={'trec_topic_number':object})\n",
    "run.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Run Info with Abstract, Title, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractsRun = run.merge(abstracts, left_on=['trec_doc_id'], right_on=['trec_doc_id'], how='left')\n",
    "abstractsRun.drop([\"order\", \"x\"], axis=1, inplace=True)\n",
    "abstractsRun.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runName = run[\"run_name\"].unique()\n",
    "runName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Information from Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsColumns = ['trec_topic_number', 'trec_topic_disease', 'trec_topic_age', 'trec_topic_sex']\n",
    "topics = pd.DataFrame(columns=topicsColumns)\n",
    "topicsXML = etree.parse(\"../resources/topics2017.xml\")\n",
    "for topic in topicsXML.getroot():\n",
    "    topicNumber = topic.get('number')\n",
    "    disease = topic.find('disease').text\n",
    "    demographic = topic.find('demographic').text.split(' ')\n",
    "    age = demographic[0]\n",
    "    sex = demographic[1]\n",
    "    topics = topics.append(pd.Series([topicNumber, disease, age, sex], index=topicsColumns), ignore_index=True)\n",
    "topics.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add topics Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedRun = abstractsRun.merge(topics, left_on=['trec_topic_number'], right_on=['trec_topic_number'], how='left')\n",
    "processedRun['relevance_score'] = 0\n",
    "processedRun.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Result into a new _.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = time.strftime(\"%Y%m%d\")\n",
    "processedRun.to_csv(path_or_buf='../results/runs/'+ date + 'processed'+runName[0]+'.tsv', index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
