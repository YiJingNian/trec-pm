{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "* Annotations: https://drive.google.com/file/d/1IH4dL4OKG7bv57K8DreOeSAfJgkgC4sd/view\n",
    "* Relevance score: http://www.trec-cds.org/qrels-treceval-abstracts.2017.txt\n",
    "* Pubmed TXT collections: http://trec-cds.appspot.com/2018.html#documents\n",
    "* 2017 Topics: http://trec-cds.appspot.com/topics2017.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, walk\n",
    "from os.path import isfile, isdir, join, dirname\n",
    "from collections import OrderedDict\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import tarfile\n",
    "import time\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGzFileNames(myPath):\n",
    "    fileNames = [f for f in listdir(myPath) if isfile(join(myPath, f)) and f[-3:] == \".gz\"]\n",
    "    return fileNames\n",
    "\n",
    "def decompressTarGz(myPath):\n",
    "    fileNames = [f for f in listdir(myPath) if isfile(join(myPath, f)) and f[-7:] == \".tar.gz\"]\n",
    "    for file in fileNames:\n",
    "        print(\"Extracting from:\", file)\n",
    "        tar = tarfile.open(join(myPath, file), \"r:gz\")\n",
    "        tar.extractall(join(myPath, file[:-7]))\n",
    "        tar.close()\n",
    "        print(\"Done\")\n",
    "        \n",
    "# Extract Ids from the Gold-Standard CSV File\n",
    "def extractDocIDs(filePath):\n",
    "    f = pd.read_csv(filePath)\n",
    "    return set(f['trec_doc_id'])\n",
    "\n",
    "def unzipTar(folderPath, docIDsPath, targetFolder=''):\n",
    "    # Unzip either pubmed or extra abstracts from folderPath to targetFolder if they are in the gold standard\n",
    "    ids = extractDocIDs(docIDsPath)\n",
    "    print(\"Gold Standard Ids:\", len(ids))\n",
    "    tarFiles = getTarFileNames(folderPath)\n",
    "    \n",
    "    if targetFolder:\n",
    "        outpuPath=join(folderPath, targetFolder)\n",
    "    else:\n",
    "        outpuPath=folderPath\n",
    "    txtCounter = 0\n",
    "    for tarFileName in tarFiles:\n",
    "        print(\"Searching through:\", tarFileName)\n",
    "        tar = tarfile.open(join(folderPath, tarFileName), 'r:')\n",
    "        for txtFile in tar:\n",
    "            \n",
    "            # Extract ID from full path\n",
    "            docID = re.search( r'\\/(.*)\\.', txtFile.name)\n",
    "            if docID:\n",
    "                # Extract file only when there is a match\n",
    "                if docID.group(1) in ids:\n",
    "                    txtCounter += 1\n",
    "                    ids.remove(docID.group(1))\n",
    "                    tar.extract(txtFile, path=outpuPath)\n",
    "\n",
    "        tar.close()\n",
    "    print(\"Matched files:\", txtCounter)\n",
    "\n",
    "def getTarFileNames(myPath):\n",
    "    fileNames = [f for f in listdir(myPath) if isfile(join(myPath, f)) and f[-4:] == \".tar\"]\n",
    "    return fileNames\n",
    "\n",
    "def extractFeatures(folderPath, extractName):\n",
    "    st = time.time()\n",
    "    \n",
    "    folders = [fo for fo in listdir(folderPath) if isdir(join(folderPath, fo))]\n",
    "    with open(extractName, 'w') as extractFile:\n",
    "         wr = csv.writer(extractFile, quoting=csv.QUOTE_ALL, delimiter=\"\\t\")\n",
    "         wr.writerow([\"id\",\"title\",\"abstract\"])\n",
    "    fCount = 0\n",
    "    for fo in folders:\n",
    "        print(\"fo: \", fo)\n",
    "        filesInFo = [fi for fi in listdir(join(folderPath, fo)) if isfile(join(folderPath, fo, fi))]\n",
    "        for fi in filesInFo:\n",
    "            fCount += 1\n",
    "            # print(\"fi: \", fi)\n",
    "            fiObj = open(join(folderPath, fo, fi), encoding=\"utf8\")\n",
    "            fId = fi[:-4]\n",
    "            lines = fiObj.readlines()\n",
    "            title = lines[0].strip()\n",
    "            abstract = \"\"\n",
    "            for line in lines[1:]:\n",
    "                if line.strip():\n",
    "                    abstract += line.strip() + \" \"\n",
    "            with open(extractName, 'a', encoding=\"utf8\") as extractFile:\n",
    "                wr = csv.writer(extractFile, quoting=csv.QUOTE_ALL, delimiter=\"\\t\")\n",
    "                wr.writerow([fId, title, abstract])\n",
    "    print(\"Extracted files:\", fCount)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"time: \", end-st)\n",
    "\n",
    "def extractExtraFeatures(extraAbstracts, extractedFeaturesFile):\n",
    "    files = [fi for fi in listdir(extraAbstracts) if isfile(join(extraAbstracts, fi))]\n",
    "    fCount = 0\n",
    "    for fi in files:\n",
    "        fCount += 1\n",
    "        fiObj = open(join(extraAbstracts, fi), encoding=\"utf8\")\n",
    "        fId = fi[:-4]\n",
    "        lines = fiObj.readlines()\n",
    "        fullTitle = lines[1].strip()\n",
    "        title = re.search( r'(Title:) (.*)', fullTitle).group(2)\n",
    "        abstract = \"\"\n",
    "        for line in lines[2:]:\n",
    "            if line.strip():\n",
    "                abstract += line.strip() + \" \"\n",
    "        with open(extractedFeaturesFile, 'a', encoding=\"utf8\") as extractFile:\n",
    "            wr = csv.writer(extractFile, quoting=csv.QUOTE_ALL, delimiter=\"\\t\")\n",
    "            wr.writerow([fId, title, abstract])    \n",
    "    print(\"Extracted files:\", fCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Pubmed Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path containing the downloaded txt collection\n",
    "pubmedAbstractsPath = \"/TREC/TXT-Collection/pubmed\"\n",
    "# Path containing the extracted txt collection\n",
    "pubmedAbstractsNewPath = join(pubmedAbstractsPath,\"medline_txt/medline_txt\")\n",
    "# Path containing the Annotated Gold-Standard File\n",
    "goldIDsPath = \"../resources/gold-docs-annotations\"\n",
    "# Output file\n",
    "extractedFeaturesFile = \"../resources/relevant-abstracts-TXT.csv\"\n",
    "\n",
    "# Decompress files\n",
    "decompressTarGz(pubmedAbstractsPath)\n",
    "abstractsGzFiles = getGzFileNames(pubmedAbstractsNewPath)\n",
    "\n",
    "for abstractsGzFile in abstractsGzFiles:\n",
    "    print(\"Extracting: \", abstractsGzFile)\n",
    "    subprocess.call(['gunzip', '-d', join(pubmedAbstractsNewPath, abstractsGzFile)])\n",
    "    print(\"Done\")\n",
    "    \n",
    "unzipTar(pubmedAbstractsNewPath, goldIDsPath, \"Gold\")\n",
    "extractFeatures(pubmedAbstractsNewPath + \"/Gold\", extractedFeaturesFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Extra Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path containing the downloaded txt collection (extra abstracts)\n",
    "extraAbstractsPath = \"/TREC/TXT-Collection/extra_abstracts\"\n",
    "# Path containing the extracted txt collection (extra abstracts)\n",
    "extraAbstractsNewPath = join(extraAbstractsPath,\"extra_abstracts\")\n",
    "abstractsGzFiles = getGzFileNames(extraAbstractsPath)\n",
    "\n",
    "for abstractsGzFile in abstractsGzFiles:\n",
    "    print(\"Extracting: \", abstractsGzFile)\n",
    "    subprocess.call(['gunzip', '-d', join(abstractsPath, abstractsGzFile)])\n",
    "    print(\"Done\")\n",
    "\n",
    "unzipTar(extraAbstractsPath, goldIDsPath)\n",
    "extractExtraFeatures(extraAbstractsNewPath, extractedFeaturesFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtAbstracts = pd.read_csv(\"../resources/relevant-abstracts-TXT.csv\", sep='\\t', header=None, names=[\"trec_doc_id\", \"title\", \"abstract\"], dtype={'trec_doc_id':object})\n",
    "txtAbstracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(\"../resources/gold-doc-IDs.csv\", sep=',', encoding=\"utf-8\", dtype={'trec_topic_number':object})\n",
    "annotations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Relevance Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance = pd.read_csv(\"../resources/qrels-treceval-abstracts.2017.txt\", sep=' ', encoding=\"utf-8\", header=None, \n",
    "                        names=[\"trec_topic_number\", \"x\", \"trec_doc_id\", \"relevance_score\"], dtype={'trec_topic_number':object})\n",
    "relevance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsColumns = ['trec_topic_number', 'trec_topic_disease', 'trec_topic_age', 'trec_topic_sex', 'trec_topic_other1', 'trec_topic_other2', 'trec_topic_other3']\n",
    "topics = pd.DataFrame(columns=topicsColumns)\n",
    "topicsXML = etree.parse(\"../resources/topics2017.xml\")\n",
    "for topic in topicsXML.getroot():\n",
    "    topicNumber = topic.get('number')\n",
    "    disease = topic.find('disease').text\n",
    "    demographic = topic.find('demographic').text.split(' ')\n",
    "    age = demographic[0]\n",
    "    sex = demographic[1]\n",
    "    other = topic.find('other').text.split(',')\n",
    "    other1 = other[0]\n",
    "    other2 = None\n",
    "    other3 = None\n",
    "    if len(other) == 2:\n",
    "        other2 = other[1]\n",
    "    if len(other) > 2:\n",
    "        other3 = other[2]\n",
    "    topics = topics.append(pd.Series([topicNumber, disease, age, sex, other1, other2, other3], index=topicsColumns), ignore_index=True)\n",
    "topics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicsColumns = ['trec_topic_number', 'trec_topic_disease', 'trec_topic_age', 'trec_topic_sex']\n",
    "topics = pd.DataFrame(columns=topicsColumns)\n",
    "topicsXML = etree.parse(\"../resources/topics2018.xml\")\n",
    "for topic in topicsXML.getroot():\n",
    "    topicNumber = topic.get('number')\n",
    "    disease = topic.find('disease').text\n",
    "    demographic = topic.find('demographic').text.split(' ')\n",
    "    age = demographic[0]\n",
    "    sex = demographic[1]\n",
    "    topics = topics.append(pd.Series([topicNumber, disease, age, sex], index=topicsColumns), ignore_index=True)\n",
    "topics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Relevance Score and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotationsRelevance = annotations.merge(relevance, left_on=['trec_topic_number','trec_doc_id'], right_on=['trec_topic_number','trec_doc_id'], how='left')\n",
    "annotationsRelevance.drop([\"x\"], axis=1, inplace=True)\n",
    "annotationsRelevance.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Abstracts with Relevance Score and Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedAbstracts = annotationsRelevance.merge(txtAbstracts, left_on=['trec_doc_id'], right_on=['trec_doc_id'], how='left')\n",
    "processedAbstracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add 2017 Topics Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedGoldStandard = processedAbstracts.merge(topics, left_on=['trec_topic_number'], right_on=['trec_topic_number'], how='left')\n",
    "processedGoldStandard.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Result into a new _.csv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = time.strftime(\"%Y%m%d\")\n",
    "processedGoldStandard.to_csv(path_or_buf='../resources/'+ date + 'processed-goldstandard-TXT.csv', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
