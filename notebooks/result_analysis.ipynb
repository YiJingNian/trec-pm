{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDirectory = \"../results/analysis/\"\n",
    "\n",
    "if not os.path.exists(outputDirectory):\n",
    "    os.makedirs(outputDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "filePath = \"../docs/2018/final-results/biomedical-articles\"\n",
    "outputPath = \"../results/analysis/biomedical-articles\"\n",
    "\n",
    "if not os.path.exists(outputPath):\n",
    "    os.makedirs(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGraph(name, metric):\n",
    "    ext = \"trec_eval\"\n",
    "    if(metric == \"infNDCG\"):\n",
    "        ext = \"sampleval\"\n",
    "\n",
    "    col_names = ['metric','topic','value']\n",
    "    df = pd.read_csv(join(filePath, name + \".\" + ext), sep=r'\\t+', header=None, names=col_names, \n",
    "                     dtype={'value':float}, engine='python')\n",
    "    filtered = df.loc[df['metric'].str.match(metric, case=False) & ~df['topic'].str.match(\"all\", case=False)]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(filtered['topic'], filtered['value'])\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.title(metric + \" \" + name);\n",
    "    plt.axhline(filtered['value'].mean(), color='r', label=\"Mean: all topics\");\n",
    "    plt.savefig(join(outputPath,'experiment-'+name+\"-\"+metric+'.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeMetricGraph(metric):\n",
    "    ext = \"trec_eval\"\n",
    "    if(metric == \"infNDCG\"):\n",
    "         ext = \"sampleval\"\n",
    "\n",
    "    fileNames = [f for f in listdir(filePath) if isfile(join(filePath, f)) and f[-9:] == ext]\n",
    "\n",
    "    colors=[\"red\",\"blue\",\"black\",\"yellow\",\"purple\"]\n",
    "    markers=[\"o\",\"^\",\"*\",\"X\",\"p\"]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.title(metric)\n",
    "\n",
    "    for file in fileNames:\n",
    "        col_names = ['metric','topic','value']\n",
    "        df = pd.read_csv(join(filePath, file), sep=r'\\t+', header=None, names=col_names, \n",
    "                         dtype={'value':float}, engine='python')\n",
    "        filtered = df.loc[df['metric'].str.match(metric, case=False) & ~df['topic'].str.match(\"all\", case=False)]\n",
    "        allTopics = df.loc[df['metric'].str.match(metric, case=False) & df['topic'].str.match(\"all\", case=False)]['value']\n",
    "        plt.scatter(filtered['topic'], filtered['value'], c=colors.pop(), alpha=0.5, marker=markers.pop(), label=file[:-10])\n",
    "\n",
    "    plt.axhline(allTopics.mean(), color='r', label=\"Mean: all topics & runs\");\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(join(outputPath,'experiment-'+metric+'.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTopicGraph(metric):\n",
    "    ext = \"trec_eval\"\n",
    "    if(metric == \"infNDCG\"):\n",
    "         ext = \"sampleval\"\n",
    "\n",
    "    fileNames = [f for f in listdir(filePath) if isfile(join(filePath, f)) and f[-9:] == ext]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(metric)\n",
    "\n",
    "    allRuns = pd.DataFrame()\n",
    "    for file in fileNames:\n",
    "        col_names = ['metric','topic','value']\n",
    "        df = pd.read_csv(join(filePath, file), sep=r'\\t+', header=None, names=col_names, \n",
    "                         dtype={'value':float}, engine='python')\n",
    "        filtered = df.loc[df['metric'].str.match(metric, case=False) & ~df['topic'].str.match(\"all\", case=False)]\n",
    "        allTopics = df.loc[df['metric'].str.match(metric, case=False) & df['topic'].str.match(\"all\", case=False)]['value']\n",
    "        # For calculating the max, mean and min per topic     \n",
    "        if allRuns.empty:\n",
    "            allRuns = filtered\n",
    "        else:\n",
    "            allRuns = allRuns.append(filtered)\n",
    "    \n",
    "    maxDf = allRuns.groupby('topic', as_index=False)['value'].max().astype(float).sort_values(by=['topic']);\n",
    "    minDf = allRuns.groupby('topic', as_index=False)['value'].min().astype(float).sort_values(by=['topic']);\n",
    "    meanDf = allRuns.groupby('topic', as_index=False)['value'].mean().astype(float).sort_values(by=['topic']);\n",
    "    \n",
    "    plt.axhline(allTopics.mean(), color='violet', label=\"Mean: all topics & runs\", linewidth=\"1\");\n",
    "    plt.scatter(maxDf['topic'], maxDf['value'], c='g', label='Max', marker = \"+\")\n",
    "    plt.scatter(meanDf['topic'], meanDf['value'], c='r', label='Mean', marker = \"_\")\n",
    "    plt.scatter(minDf['topic'], minDf['value'], c='b', label='Min', marker = \".\")    \n",
    "    plt.legend(loc='best')\n",
    "    plt.xticks(maxDf['topic'],rotation='vertical')\n",
    "    plt.savefig(join(outputPath,'experiment-topics-'+metric+'.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeMetricGraph(metric):\n",
    "    ext = \"trec_eval\"\n",
    "    if(metric == \"infNDCG\"):\n",
    "         ext = \"sampleval\"\n",
    "\n",
    "    fileNames = [f for f in listdir(filePath) if isfile(join(filePath, f)) and f[-9:] == ext]\n",
    "\n",
    "    colors=[\"red\",\"blue\",\"black\",\"yellow\",\"purple\"]\n",
    "    markers=[\"o\",\"^\",\"*\",\"X\",\"p\"]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.title(metric)\n",
    "\n",
    "    for file in fileNames:\n",
    "        col_names = ['metric','topic','value']\n",
    "        df = pd.read_csv(join(filePath, file), sep=r'\\t+', header=None, names=col_names, \n",
    "                         dtype={'value':float}, engine='python')\n",
    "        filtered = df.loc[df['metric'].str.match(metric, case=False) & ~df['topic'].str.match(\"all\", case=False)]\n",
    "        allTopics = df.loc[df['metric'].str.match(metric, case=False) & df['topic'].str.match(\"all\", case=False)]['value']\n",
    "        plt.scatter(filtered['topic'], filtered['value'], c=colors.pop(), alpha=0.5, marker=markers.pop(), label=file[:-10])\n",
    "\n",
    "    plt.axhline(allTopics.mean(), color='r', label=\"Mean: all topics & runs\");\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(join(outputPath,'experiment-'+metric+'.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biomedical Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=[\"P10 \",\"R-prec\",\"infNDCG\"]\n",
    "runs=[\"hpipubcommon\",\"hpipubbase\",\"hpipubnone\",\"hpipubclass\",\"hpipubboost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "    for metric in metrics:\n",
    "        makeGraph(run, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    makeMetricGraph(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    makeTopicGraph(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    makeMetricGraphWithRunInfo(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "filePath = \"../docs/2018/final-results/clinical-trials\"\n",
    "outputPath = \"../results/analysis/clinical-trials\"\n",
    "\n",
    "if not os.path.exists(outputPath):\n",
    "    os.makedirs(outputPath)\n",
    "\n",
    "metrics=[\"P10\",\"R-prec\",\"infNDCG\"]\n",
    "runs=[\"hpictcommon\",\"hpictbase\",\"hpictall\",\"hpictphrase\",\"hpictboost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "    for metric in metrics:\n",
    "        makeGraph(run, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    makeMetricGraph(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "    makeTopicGraph(metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
