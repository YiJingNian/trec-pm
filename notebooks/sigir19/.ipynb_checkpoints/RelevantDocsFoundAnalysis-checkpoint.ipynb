{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goldfile   = \"../../resources/20180622processedGoldStandardTopics.tsv.gz\"\n",
    "resultsdir = \"../../results/\"\n",
    "statsdir   = \"../../stats_pmclass/\"\n",
    "measures   = [\"ndcg\",\"infNDCG\", \"P_10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "allgsdf = pd.read_csv(goldfile, delimiter=\"\\t\")\n",
    "gsdf = allgsdf.drop([\"title\", \"abstract\", \"major_mesh\", \"minor_mesh\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRelFoundCounts(dir):\n",
    "    # Read all results in the given result directory\n",
    "    resultfiles = sorted(os.listdir(dir))\n",
    "    # Read the result files as DataFrames into a map\n",
    "    resultdfmap = []\n",
    "    for f in resultfiles:\n",
    "        resultdfmap.append(pd.read_csv(dir+f, delimiter=\"\\t\", names=[\"topic\", \"Q0\", \"docid\", \"rank\", \"score\", \"run\"]))\n",
    "    # Create a DataFrame multiindexed with the file name (because those are the keys of the DF maps)\n",
    "    resultmultidxdf = pd.concat(resultdfmap)\n",
    "    resultmultidxdf.set_index([\"run\"], inplace=True)\n",
    "    \n",
    "    # Create a duplication of the relevant GS document to match the results\n",
    "    gsreldocs = gsdf.query(\"relevance_score > 0\")[[\"trec_topic_number\", \"trec_doc_id\"]]\n",
    "    l = []\n",
    "    for experiment in set(resultmultidxdf.index):\n",
    "        idx = pd.Index([experiment]*len(gsreldocs), name=\"run\")\n",
    "        gscopy = gsreldocs.copy()\n",
    "        gscopy.index = idx\n",
    "        l.append(gscopy)\n",
    "    gsdfs = pd.concat(l)\n",
    "    \n",
    "    # Merge the duplicated GS with the results\n",
    "    # With a `left` join, thus eliminating all irrelevant documents.\n",
    "    relmerge = pd.merge(gsdfs, resultmultidxdf, how=\"left\", left_on=[\"run\", \"trec_topic_number\", \"trec_doc_id\"], right_on=[\"run\", \"topic\", \"docid\"])\n",
    "    relmerge.set_index(\"trec_topic_number\", append=\"True\", inplace=True)\n",
    "    \n",
    "    # Count the number of found documents per run and topic    \n",
    "    countsruntopic = relmerge.groupby([\"run\", \"trec_topic_number\"]).count()\n",
    "    countsruntopic = countsruntopic.drop([\"Q0\", \"docid\", \"rank\", \"score\"],axis=1)\n",
    "    countsruntopic.columns = [\"relgs\", \"relfound\"]\n",
    "    \n",
    "    # return the left-merged data and the counts\n",
    "    return relmerge,countsruntopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareStats(path):\n",
    "    \"\"\"\n",
    "    Reads a single stats CSV file, excludes the 'all' row and converts the topic numbers to ints.\n",
    "    Then sets the Topic columns as the new index.\n",
    "    Returns a DataFrame indexed by the non-'all' topics.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path).query(\"Topic != 'all'\")\n",
    "    df[\"Topic\"] = df[\"Topic\"].astype(int)\n",
    "    df.sort_values(by=\"Topic\", inplace=True)\n",
    "    df = df.set_index(\"Topic\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanStatsPerRun(statspath):\n",
    "    \"\"\"\n",
    "    Reads a directory of stat CSV files. Concatenates all the DataFrames and calculates the means for all\n",
    "    score measurements of the topics per run, effectively returning the 'all' row for each run. Note,\n",
    "    however, that really just the mean over the measures is given which should be the 'all' value but the actual\n",
    "    'all' value is not used here.\n",
    "    Returns only those measures defined in the 'measures' list at the beginning of this cell.\n",
    "    \"\"\"\n",
    "    statfiles = sorted(list(filter(lambda f: f.endswith(\".csv\") in f, os.listdir(statspath))))\n",
    "    runstatsmap = {}\n",
    "    for stat in statfiles:\n",
    "        df = prepareStats(statspath+stat)\n",
    "        run = stat.replace(\"OFFICIAL_\", \"\").replace(\".csv\", \"\")\n",
    "        runstatsmap[run] = df\n",
    "    allstats = pd.concat(runstatsmap)\n",
    "    allstats.index.names = [\"run\", \"Topic\"]\n",
    "    allstats = allstats[measures]\n",
    "    meanstats = allstats.mean(level=\"run\")\n",
    "    return meanstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelFoundWithMeanRank(resultspath, statspath):\n",
    "    gsleftmerged,counts = calculateRelFoundCounts(resultspath)\n",
    "    meanstats = getMeanStatsPerRun(statspath)\n",
    "    counts   = counts.sum(level=\"run\").sort_values(\"relfound\")\n",
    "    merge     = pd.merge(counts, meanstats, on=\"run\")\n",
    "    meanranks = gsleftmerged[\"rank\"].dropna().mean(level=\"run\")\n",
    "    stdranks  = gsleftmerged[\"rank\"].dropna().std(level=\"run\")\n",
    "    merge[\"meanrank\"] = meanranks\n",
    "    merge[\"stdrank\"] = stdranks\n",
    "    return merge.sort_values(\"relfound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../results/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-603cd3b1ab2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRelFoundWithMeanRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../results/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"../../stats_pmclass/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-195-8e1d050a69f3>\u001b[0m in \u001b[0;36mgetRelFoundWithMeanRank\u001b[0;34m(resultspath, statspath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetRelFoundWithMeanRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultspath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatspath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mgsleftmerged\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculateRelFoundCounts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultspath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmeanstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetMeanStatsPerRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatspath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcounts\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relfound\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmerge\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeanstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-173-15da1146f2e3>\u001b[0m in \u001b[0;36mcalculateRelFoundCounts\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculateRelFoundCounts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Read all results in the given result directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresultfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Read the result files as DataFrames into a map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresultdfmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../results/'"
     ]
    }
   ],
   "source": [
    "df = getRelFoundWithMeanRank(resultsdir, statsdir)\n",
    "print(df.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
